{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import socket\n",
    "import shutil\n",
    "import random\n",
    "import argparse\n",
    "import importlib\n",
    "import data_utils\n",
    "import scipy.misc\n",
    "import statistics \n",
    "import numpy as np\n",
    "import pointfly as pf\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC CONFIGURATION #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append python path\n",
    "PATH_TRAIN = '../data/modelnet/train_files.txt'\n",
    "PATH_VALID = '../data/modelnet/test_files.txt'\n",
    "MODEL_NAME = 'pointcnn_cls'\n",
    "SETTINGS_FILE = 'modelnet_x3_l4'\n",
    "DISCARD_NORMAL = True\n",
    "\n",
    "MODELS = ['../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-08-15-29-50_14671/ckpts/iter-78847',\n",
    "          '../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-09-07-34-26_9606/ckpts/iter-78847',\n",
    "          '../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-09-23-39-54_31034/ckpts/iter-78847',\n",
    "          '../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-10-15-38-57_22597/ckpts/iter-78847',\n",
    "          '../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-11-07-39-40_11705/ckpts/iter-78847',\n",
    "          '../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-12-11-10-48_3173/ckpts/iter-78847',\n",
    "          '../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-13-03-20-06_27936/ckpts/iter-78847',\n",
    "          '../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-13-19-33-27_20423/ckpts/iter-78847',\n",
    "          '../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-14-11-40-52_11569/ckpts/iter-78847',\n",
    "          '../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-15-03-51-29_2197/ckpts/iter-78847',]\n",
    "\n",
    "# Import model\n",
    "model = importlib.import_module(MODEL_NAME)\n",
    "\n",
    "# Import settings\n",
    "setting_path = os.path.join(str(Path().resolve()), MODEL_NAME)\n",
    "sys.path.append(setting_path)\n",
    "setting = importlib.import_module(SETTINGS_FILE)\n",
    "\n",
    "# List all settings\n",
    "num_epochs = setting.num_epochs\n",
    "batch_size = setting.batch_size\n",
    "sample_num = setting.sample_num\n",
    "step_val = setting.step_val\n",
    "rotation_range = setting.rotation_range\n",
    "rotation_range_val = setting.rotation_range_val\n",
    "scaling_range = setting.scaling_range\n",
    "scaling_range_val = setting.scaling_range_val\n",
    "jitter = setting.jitter\n",
    "jitter_val = setting.jitter_val\n",
    "pool_setting_val = None if not hasattr(setting, 'pool_setting_val') else setting.pool_setting_val\n",
    "pool_setting_train = None if not hasattr(setting, 'pool_setting_train') else setting.pool_setting_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-18 09:41:12.989211-Preparing datasets...\n",
      "2019-07-18 09:41:17.392704-(9840, 2048, 3)/(2468, 2048, 3) training/validation shapes.\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs\n",
    "print('{}-Preparing datasets...'.format(datetime.now()))\n",
    "data_train, label_train, data_val, label_val = setting.load_fn(PATH_TRAIN, PATH_VALID)\n",
    "\n",
    "# Balance\n",
    "if setting.balance_fn is not None:\n",
    "    print('{}-Balancing datasets...'.format(datetime.now()))\n",
    "    num_train_before_balance = data_train.shape[0]\n",
    "    repeat_num = setting.balance_fn(label_train)\n",
    "    data_train = np.repeat(data_train, repeat_num, axis=0)\n",
    "    label_train = np.repeat(label_train, repeat_num, axis=0)\n",
    "    data_train, label_train = data_utils.grouped_shuffle([data_train, label_train])\n",
    "    num_epochs = math.floor(num_epochs * (num_train_before_balance / data_train.shape[0]))\n",
    "\n",
    "# Save ply\n",
    "if setting.save_ply_fn is not None:\n",
    "    print('{}-Saving ply...'.format(datetime.now()))\n",
    "    folder = os.path.join(root_folder, 'pts')\n",
    "    print('{}-Saving samples as .ply files to {}...'.format(datetime.now(), folder))\n",
    "    sample_num_for_ply = min(512, data_train.shape[0])\n",
    "    if setting.map_fn is None:\n",
    "        data_sample = data_train[:sample_num_for_ply]\n",
    "    else:\n",
    "        data_sample_list = []\n",
    "        for idx in range(sample_num_for_ply):\n",
    "            data_sample_list.append(setting.map_fn(data_train[idx], 0)[0])\n",
    "        data_sample = np.stack(data_sample_list)\n",
    "    setting.save_ply_fn(data_sample, folder)\n",
    "\n",
    "if DISCARD_NORMAL:\n",
    "    data_train = data_train[..., :3]\n",
    "    data_val = data_val[..., :3]\n",
    "\n",
    "num_train = data_train.shape[0]\n",
    "point_num = data_train.shape[1]\n",
    "num_val = data_val.shape[0]\n",
    "print('{}-{}/{} training/validation shapes.'.format(datetime.now(), data_train.shape, data_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLACEHOLDERS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-07-18 09:41:23.516817-Keep remainder...\n",
      "2019-07-18 09:41:23.524713-78848 training batches.\n",
      "2019-07-18 09:41:23.531856-20 testing batches per test.\n"
     ]
    }
   ],
   "source": [
    "# Placeholders\n",
    "indices = tf.placeholder(tf.int32, shape=(None, None, 2), name=\"indices\")\n",
    "xforms = tf.placeholder(tf.float32, shape=(None, 3, 3), name=\"xforms\")\n",
    "rotations = tf.placeholder(tf.float32, shape=(None, 3, 3), name=\"rotations\")\n",
    "jitter_range = tf.placeholder(tf.float32, shape=(1), name=\"jitter_range\")\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "data_train_placeholder = tf.placeholder(data_train.dtype, data_train.shape, name='data_train')\n",
    "label_train_placeholder = tf.placeholder(tf.int64, label_train.shape, name='label_train')\n",
    "data_val_placeholder = tf.placeholder(data_val.dtype, data_val.shape, name='data_val')\n",
    "label_val_placeholder = tf.placeholder(tf.int64, label_val.shape, name='label_val')\n",
    "handle = tf.placeholder(tf.string, shape=[], name='handle')\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((data_train_placeholder, label_train_placeholder))\n",
    "dataset_train = dataset_train.shuffle(buffer_size=batch_size * 4)\n",
    "\n",
    "if setting.map_fn is not None:\n",
    "    print('{}-Map function...'.format(datetime.now()))\n",
    "    dataset_train = dataset_train.map(lambda data, label:\n",
    "                                      tuple(tf.py_func(setting.map_fn, [data, label], [tf.float32, label.dtype])),\n",
    "                                      num_parallel_calls=setting.num_parallel_calls)\n",
    "\n",
    "if setting.keep_remainder:\n",
    "    print('{}-Keep remainder...'.format(datetime.now()))\n",
    "    dataset_train = dataset_train.batch(batch_size)\n",
    "    batch_num_per_epoch = math.ceil(num_train / batch_size)\n",
    "else:\n",
    "    print('{}-Dont keep remainder...'.format(datetime.now()))\n",
    "    dataset_train = dataset_train.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n",
    "    batch_num_per_epoch = math.floor(num_train / batch_size)\n",
    "dataset_train = dataset_train.repeat(num_epochs)\n",
    "iterator_train = dataset_train.make_initializable_iterator()\n",
    "batch_num = batch_num_per_epoch * num_epochs\n",
    "print('{}-{:d} training batches.'.format(datetime.now(), int(batch_num)))\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((data_val_placeholder, label_val_placeholder))\n",
    "if setting.map_fn is not None:\n",
    "    dataset_val = dataset_val.map(lambda data, label: tuple(tf.py_func(\n",
    "        setting.map_fn, [data, label], [tf.float32, label.dtype])), num_parallel_calls=setting.num_parallel_calls)\n",
    "if setting.keep_remainder:\n",
    "    dataset_val = dataset_val.batch(batch_size)\n",
    "    batch_num_val = math.ceil(num_val / batch_size)\n",
    "else:\n",
    "    dataset_val = dataset_val.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n",
    "    batch_num_val = math.floor(num_val / batch_size)\n",
    "iterator_val = dataset_val.make_initializable_iterator()\n",
    "print('{}-{:d} testing batches per test.'.format(datetime.now(), int(batch_num_val)))\n",
    "\n",
    "iterator = tf.data.Iterator.from_string_handle(handle, dataset_train.output_types)\n",
    "(pts_fts, labels) = iterator.get_next()\n",
    "\n",
    "pts_fts_sampled = tf.gather_nd(pts_fts, indices=indices, name='pts_fts_sampled')\n",
    "features_augmented = None\n",
    "if setting.data_dim > 3:\n",
    "    points_sampled, features_sampled = tf.split(pts_fts_sampled,\n",
    "                                                [3, setting.data_dim - 3],\n",
    "                                                axis=-1,\n",
    "                                                name='split_points_features')\n",
    "    if setting.use_extra_features:\n",
    "        if setting.with_normal_feature:\n",
    "            if setting.data_dim < 6:\n",
    "                print('Only 3D normals are supported!')\n",
    "                exit()\n",
    "            elif setting.data_dim == 6:\n",
    "                features_augmented = pf.augment(features_sampled, rotations)\n",
    "            else:\n",
    "                normals, rest = tf.split(features_sampled, [3, setting.data_dim - 6])\n",
    "                normals_augmented = pf.augment(normals, rotations)\n",
    "                features_augmented = tf.concat([normals_augmented, rest], axis=-1)\n",
    "        else:\n",
    "            features_augmented = features_sampled\n",
    "else:\n",
    "    points_sampled = pts_fts_sampled\n",
    "points_augmented = pf.augment(points_sampled, xforms, jitter_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/daniel/eclipse-workspace/pointcnn/pointfly.py:144: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/daniel/eclipse-workspace/pointcnn/pointfly.py:347: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/daniel/eclipse-workspace/pointcnn/pointfly.py:303: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From /home/daniel/eclipse-workspace/pointcnn/pointfly.py:339: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/daniel/eclipse-workspace/pointcnn/pointfly.py:315: separable_conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.separable_conv2d instead.\n",
      "WARNING:tensorflow:From /home/daniel/eclipse-workspace/pointcnn/pointcnn.py:158: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Tensor(\"metrics/mean/value:0\", shape=(), dtype=float32) Tensor(\"metrics/mean/update_op:0\", shape=(), dtype=float32)\n",
      "Tensor(\"metrics/accuracy/value:0\", shape=(), dtype=float32) Tensor(\"metrics/accuracy/update_op:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:1022: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Tensor(\"metrics/mean_accuracy/mean_accuracy:0\", shape=(), dtype=float32) Tensor(\"metrics/mean_accuracy/update_op:0\", shape=(40,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "net = model.Net(points=points_augmented, features=features_augmented, is_training=is_training, setting=setting)\n",
    "logits = net.logits\n",
    "probs = tf.nn.softmax(logits, name='probs')\n",
    "predictions = tf.argmax(probs, axis=-1, name='predictions')\n",
    "\n",
    "labels_2d = tf.expand_dims(labels, axis=-1, name='labels_2d')\n",
    "labels_tile = tf.tile(labels_2d, (1, tf.shape(logits)[1]), name='labels_tile')\n",
    "loss_op = tf.losses.sparse_softmax_cross_entropy(labels=labels_tile, logits=logits)\n",
    "\n",
    "with tf.name_scope('metrics'):\n",
    "    loss_mean_op, loss_mean_update_op = tf.metrics.mean(loss_op)\n",
    "    print (loss_mean_op, loss_mean_update_op)\n",
    "    t_1_acc_op, t_1_acc_update_op = tf.metrics.accuracy(labels_tile, predictions)\n",
    "    print (t_1_acc_op, t_1_acc_update_op)\n",
    "    t_1_per_class_acc_op, t_1_per_class_acc_update_op = tf.metrics.mean_per_class_accuracy(labels_tile,\n",
    "                                                                                           predictions,\n",
    "                                                                                           setting.num_class)\n",
    "    print (t_1_per_class_acc_op, t_1_per_class_acc_update_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-07-18 09:41:32.572309-Parameter number: 599340.\n"
     ]
    }
   ],
   "source": [
    "reset_metrics_op = tf.variables_initializer([var for var in tf.local_variables()\n",
    "                                             if var.name.split('/')[0] == 'metrics'])\n",
    "\n",
    "_ = tf.summary.scalar('loss/train', tensor=loss_mean_op, collections=['train'])\n",
    "_ = tf.summary.scalar('t_1_acc/train', tensor=t_1_acc_op, collections=['train'])\n",
    "_ = tf.summary.scalar('t_1_per_class_acc/train', tensor=t_1_per_class_acc_op, collections=['train'])\n",
    "\n",
    "_ = tf.summary.scalar('loss/val', tensor=loss_mean_op, collections=['val'])\n",
    "_ = tf.summary.scalar('t_1_acc/val', tensor=t_1_acc_op, collections=['val'])\n",
    "_ = tf.summary.scalar('t_1_per_class_acc/val', tensor=t_1_per_class_acc_op, collections=['val'])\n",
    "\n",
    "lr_exp_op = tf.train.exponential_decay(setting.learning_rate_base, global_step, setting.decay_steps,\n",
    "                                       setting.decay_rate, staircase=True)\n",
    "lr_clip_op = tf.maximum(lr_exp_op, setting.learning_rate_min)\n",
    "_ = tf.summary.scalar('learning_rate', tensor=lr_clip_op, collections=['train'])\n",
    "reg_loss = setting.weight_decay * tf.losses.get_regularization_loss()\n",
    "if setting.optimizer == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr_clip_op, epsilon=setting.epsilon)\n",
    "elif setting.optimizer == 'momentum':\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=lr_clip_op, momentum=setting.momentum, use_nesterov=True)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss_op + reg_loss, global_step=global_step)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "parameter_num = np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()])\n",
    "print('{}-Parameter number: {:d}.'.format(datetime.now(), parameter_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE METHOD\n",
    "\n",
    "Args:\n",
    "\n",
    "  **model_path** - path to evaluated model\n",
    "  \n",
    "  **num_votes** - how many votes (one vote is one pc rotation & permutation) should be used to eval the model\n",
    "\n",
    "Returns:\n",
    "  \n",
    "  **predictions** - array of the output ofthe classification module with shape: (N, 40), where N is the test clouds len\n",
    "  \n",
    "  **true_labels** - true labels of test clouds with the lenght of N\n",
    "  \n",
    "  **accuracy** - accuracy of specified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, num_votes, verbose=False):\n",
    "\n",
    "    # Start session\n",
    "    #new_graph = tf.Graph()\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Load the model\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path)\n",
    "        if verbose:\n",
    "            print('{}-Checkpoint loaded from {}!'.format(datetime.now(), model_path))\n",
    "\n",
    "        # Handle\n",
    "        handle_val = sess.run(iterator_val.string_handle())\n",
    "\n",
    "        # Data to remember\n",
    "        voted_logits = []\n",
    "        voted_labels = None\n",
    "\n",
    "        # Num votes range\n",
    "        for _ in range(num_votes):\n",
    "\n",
    "            # Restart dataset iterator\n",
    "            sess.run(iterator_val.initializer, feed_dict={\n",
    "                        data_val_placeholder: data_val,\n",
    "                        label_val_placeholder: label_val,\n",
    "                    })\n",
    "\n",
    "            # Data to remember\n",
    "            batch_logits = []\n",
    "            batch_labels = []\n",
    "\n",
    "            # For each batch in dataset\n",
    "            for batch_idx_val in range(int(batch_num_val)):\n",
    "\n",
    "                # Set the batchsize\n",
    "                if not setting.keep_remainder or num_val % batch_size == 0 or batch_idx_val != batch_num_val - 1:\n",
    "                    batch_size_val = batch_size\n",
    "                else:\n",
    "                    batch_size_val = num_val % batch_size\n",
    "\n",
    "                # Get the xforms and rotations\n",
    "                xforms_np, rotations_np = pf.get_xforms(batch_size_val, rotation_range=rotation_range_val,\n",
    "                                                        scaling_range=scaling_range_val, order=setting.rotation_order)\n",
    "\n",
    "                # Get logits and labels\n",
    "                pred_val, labels_val = sess.run([logits, labels_tile], feed_dict={\n",
    "                             handle: handle_val,\n",
    "                             indices: pf.get_indices(batch_size_val, sample_num, point_num),\n",
    "                             xforms: xforms_np,\n",
    "                             rotations: rotations_np,\n",
    "                             jitter_range: np.array([jitter_val]),\n",
    "                             is_training: False,\n",
    "                         })\n",
    "\n",
    "                # Remember data\n",
    "                batch_logits.append(np.squeeze(pred_val, axis=1))\n",
    "                batch_labels.append(np.squeeze(labels_val))\n",
    "\n",
    "            # Concatenate\n",
    "            batch_logits = np.concatenate(batch_logits, axis=0)\n",
    "            batch_labels = np.concatenate(batch_labels)\n",
    "            voted_logits.append(batch_logits)\n",
    "            voted_labels = batch_labels\n",
    "\n",
    "        # Final concat\n",
    "        voted_logits = np.sum(np.stack(voted_logits).transpose(1, 2, 0), axis=-1)\n",
    "        voted_predic = np.argmax(voted_logits, axis=-1)\n",
    "        voted_accura = float(np.sum(voted_predic == voted_labels)) / len(voted_labels)\n",
    "        return voted_logits, voted_labels, voted_accura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# NUM_VOTES TEST\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VOTES = 15\n",
    "RANGE_MODELS = range(10)\n",
    "num_votes_accs = {i: [] for i in RANGE_MODELS}\n",
    "for i in num_votes_accs:\n",
    "    for x in range(1, NUM_VOTES+1):\n",
    "        _, _, acc = evaluate(MODEL[i], num_votes=x, verbose=False)\n",
    "        num_votes_accs[i].append(acc)\n",
    "        print ('i=', i, 'x=', x, 'acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in num_votes_accs:\n",
    "    plt.plot(np.arange(1, 1+len(num_votes_accs[i])), num_votes_accs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_votes_accs_np = np.zeros((max(RANGE_MODELS), NUM_VOTES), dtype=np.float)\n",
    "for i in num_votes_accs:\n",
    "    num_votes_accs_np[i-1] = num_votes_accs[i]\n",
    "    np.save('log/num_votes_accs.npy', num_votes_accs_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# MODEL ENSEMBLING \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "## Calculate probabilities for each test cloud and each model. The output probability array will be the shape of (N, 40, X), where N is the test cloud len and X is the models count to be ensembled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-08-15-29-50_14671/ckpts/iter-78847\n",
      "Model = 0 acc =  0.9165316045380876\n",
      "INFO:tensorflow:Restoring parameters from ../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-09-07-34-26_9606/ckpts/iter-78847\n",
      "Model = 1 acc =  0.9149108589951378\n",
      "INFO:tensorflow:Restoring parameters from ../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-09-23-39-54_31034/ckpts/iter-78847\n",
      "Model = 2 acc =  0.9173419773095624\n",
      "INFO:tensorflow:Restoring parameters from ../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-10-15-38-57_22597/ckpts/iter-78847\n",
      "Model = 3 acc =  0.9145056726094003\n",
      "INFO:tensorflow:Restoring parameters from ../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-11-07-39-40_11705/ckpts/iter-78847\n",
      "Model = 4 acc =  0.919773095623987\n",
      "INFO:tensorflow:Restoring parameters from ../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-12-11-10-48_3173/ckpts/iter-78847\n",
      "Model = 5 acc =  0.9145056726094003\n",
      "INFO:tensorflow:Restoring parameters from ../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-13-03-20-06_27936/ckpts/iter-78847\n",
      "Model = 6 acc =  0.9136952998379254\n",
      "INFO:tensorflow:Restoring parameters from ../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-13-19-33-27_20423/ckpts/iter-78847\n",
      "Model = 7 acc =  0.9161264181523501\n",
      "INFO:tensorflow:Restoring parameters from ../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-14-11-40-52_11569/ckpts/iter-78847\n",
      "Model = 8 acc =  0.9116693679092382\n",
      "INFO:tensorflow:Restoring parameters from ../models/cls/pointcnn_cls_modelnet_x3_l4_2019-07-15-03-51-29_2197/ckpts/iter-78847\n",
      "Model = 9 acc =  0.919773095623987\n"
     ]
    }
   ],
   "source": [
    "NUM_MODELS = 10\n",
    "NUM_VOTES = 12\n",
    "probabilities = []\n",
    "true_labels = []\n",
    "accuracies = []\n",
    "for x in range(NUM_MODELS):\n",
    "    pred_vals, true_vals, acc = evaluate(MODELS[x], num_votes=NUM_VOTES, verbose=False)\n",
    "    probabilities.append(pred_vals)\n",
    "    accuracies.append(acc)\n",
    "    true_labels = np.array(true_vals)\n",
    "    print ('Model =', x, 'acc = ', acc)\n",
    "    \n",
    "probabilities = np.stack(probabilities).transpose(1, 2, 0)\n",
    "accuracies = np.array(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('probabilities.npy', probabilities)\n",
    "np.save('true_labels.npy', true_labels)\n",
    "np.save('accuracies.npy', accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models evaluation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy = 0.9158833063209076\n",
      "Mean of validation max results = 0.9214294975688817\n",
      "Mean of test result for validation max = 0.916790113452188\n"
     ]
    }
   ],
   "source": [
    "print('Mean accuracy =', statistics.mean(accuracies))\n",
    "indices = {}\n",
    "for k in range(40):\n",
    "    indices[k] = [i for i, x in enumerate(true_labels) if x == k]\n",
    "    \n",
    "validation_max_res = []\n",
    "test_res_at_validation_max = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    validation_indices = []\n",
    "    test_indices = []\n",
    "    for k in indices:\n",
    "        random.shuffle(indices[k])\n",
    "        split_idx = int(len(indices[k])/2)\n",
    "        validation_indices += indices[k][:split_idx]\n",
    "        test_indices += indices[k][split_idx:]\n",
    "    validation_indices = sorted(validation_indices)\n",
    "    test_indices = sorted(test_indices)\n",
    "\n",
    "    validation_true_labels = true_labels[validation_indices]\n",
    "    validation_probabilities = probabilities[validation_indices]\n",
    "    test_true_labels = true_labels[test_indices]\n",
    "    test_probabilities = probabilities[test_indices]\n",
    "\n",
    "    validation_predictions = np.argmax(validation_probabilities, axis=1)\n",
    "    validation_compare = np.equal(validation_predictions, np.expand_dims(validation_true_labels, -1))\n",
    "    validation_accuracies = np.mean(validation_compare, axis=0)\n",
    "\n",
    "    test_predictions = np.argmax(test_probabilities, axis=1)\n",
    "    test_compare = np.equal(test_predictions, np.expand_dims(test_true_labels, -1))\n",
    "    test_accuracies = np.mean(test_compare, axis=0)\n",
    "\n",
    "    validation_max_res.append(np.max(validation_accuracies))\n",
    "    test_res_at_validation_max.append(test_accuracies[np.argmax(validation_accuracies)])\n",
    "    \n",
    "mean_valid_max = statistics.mean(validation_max_res)\n",
    "mean_test_at_valid_max = statistics.mean(test_res_at_validation_max)\n",
    "\n",
    "print('Mean of validation max results =', mean_valid_max)\n",
    "print('Mean of test result for validation max =', mean_test_at_valid_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate the outputs with sum operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9222042139384117"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_probability = np.sum(probabilities, axis=-1)\n",
    "aggregated_predictions = np.argmax(aggregated_probability, axis=-1)\n",
    "float(np.sum(aggregated_predictions == true_labels)) / len(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate the outputs with mean operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9222042139384117"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_probability = np.mean(probabilities, axis=-1)\n",
    "aggregated_predictions = np.argmax(aggregated_probability, axis=-1)\n",
    "float(np.sum(aggregated_predictions == true_labels)) / len(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate the outputs with mode operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9213938411669368"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_predictions = np.argmax(probabilities, axis=1)\n",
    "aggregated_predictions =  np.squeeze(stats.mode(aggregated_predictions, axis=1)[0])\n",
    "float(np.sum(aggregated_predictions == true_labels)) / len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
